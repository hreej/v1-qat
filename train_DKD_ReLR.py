"""
train_distillation.py
çŸ¥è¯†è’¸é¦è®­ç»ƒè„šæœ¬: ä½¿ç”¨ DKD (Decoupled Knowledge Distillation) æ–¹æ³•
æ•™å¸ˆ: ResNet18 | å­¦ç”Ÿ: LiteNet
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import os
import time
from tqdm import tqdm
import numpy as np

from net_model import Litenet
from net_model_pretrained import LitenetResNet
from visualize import visualize_all, plot_training_curves, plot_confusion_matrix, plot_per_class_accuracy

# ==================== é…ç½® ====================
class DistillConfig:
    """è’¸é¦è®­ç»ƒé…ç½®"""
    
    # æ•°æ®è·¯å¾„
    DATASET_PATH = r"D:\study\CNN_demo\Litenet\dataset_v5"
    TRAIN_DIR = os.path.join(DATASET_PATH, "train")
    VALID_DIR = os.path.join(DATASET_PATH, "valid")
    
    # æ¨¡å‹ä¿å­˜è·¯å¾„ (æ ¹æ®å‚æ•°åŠ¨æ€å‘½å)
    CHECKPOINT_DIR = "distill_checkpoints/DKD_T3_A8_B2_ReLR"
    INDICATOR_DIR = "distill_indicator/DKD_T3_A8_B2_ReLR"
    
    # æ•™å¸ˆæ¨¡å‹è·¯å¾„
    TEACHER_CKPT = r"pre_checkpoints/resnet18_best_model.pth"
    
    # è®­ç»ƒè¶…å‚æ•°
    NUM_CLASSES = 12
    IMG_SIZE = 128
    BATCH_SIZE = 128
    EPOCHS = 180
    LEARNING_RATE = 0.001
    WEIGHT_DECAY = 1e-4
    MIN_LR = 1e-5
    PATIENCE = 20  # æ—©åœè€å¿ƒå€¼
    
    # ğŸ”¥ DKD è’¸é¦ç‰¹å®šå‚æ•°
    # å‚è€ƒæ–‡çŒ®æ¨è: T=4.0, alpha=1.0, beta=2.0 (é’ˆå¯¹ ResNet æ¶æ„)
    TEMPERATURE = 3.0  # è’¸é¦æ¸©åº¦
    DKD_ALPHA = 8.0    # TCKD æƒé‡ (ç›®æ ‡ç±»çŸ¥è¯†)
    DKD_BETA = 2.0     # NCKD æƒé‡ (éç›®æ ‡ç±»çŸ¥è¯†)
    
    # è®¾å¤‡è®¾ç½®
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # æ•°æ®åŠ è½½è®¾ç½®
    NUM_WORKERS = 2
    PIN_MEMORY = True
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    # LR_SCHEDULER = "CosineAnnealingLR"
    # LR_SCHEDULER = "MultiStepLR"
    LR_SCHEDULER = "ReduceLROnPlateau"

# ==================== æ—©åœæœºåˆ¶ ====================
class EarlyStopping:
    """æ—©åœæœºåˆ¶"""
    def __init__(self, patience=10, verbose=False, delta=0):
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.delta = delta

    def __call__(self, val_acc):
        score = val_acc
        if self.best_score is None:
            self.best_score = score
        elif score > self.best_score + self.delta:
            self.best_score = score
            self.counter = 0
        else:
            self.counter += 1
            if self.verbose:
                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True

# ==================== DKD æŸå¤±å‡½æ•° ====================
def _get_gt_mask(logits, target):
    """è¾…åŠ©å‡½æ•°: ç”Ÿæˆç›®æ ‡ç±»åˆ«çš„æ©ç """
    target = target.reshape(-1)
    mask = torch.zeros_like(logits).scatter_(1, target.unsqueeze(1), 1).bool()
    return mask

def _get_other_mask(logits, target):
    """è¾…åŠ©å‡½æ•°: ç”Ÿæˆéç›®æ ‡ç±»åˆ«çš„æ©ç """
    target = target.reshape(-1)
    mask = torch.ones_like(logits).scatter_(1, target.unsqueeze(1), 0).bool()
    return mask

def dkd_loss(logits_student, logits_teacher, target, alpha, beta, temperature):
    """
    Decoupled Knowledge Distillation Loss (CVPR 2022)
    Loss = alpha * TCKD + beta * NCKD
    """
    # è·å–æ©ç 
    gt_mask = _get_gt_mask(logits_student, target)
    other_mask = _get_other_mask(logits_student, target)
    
    # è®¡ç®—å¸¦æ¸©åº¦çš„ Softmax
    pred_student = F.softmax(logits_student / temperature, dim=1)
    pred_teacher = F.softmax(logits_teacher / temperature, dim=1)
    
    # ----------- TCKD (Target Class Knowledge Distillation) -----------
    # æ„é€ äºŒåˆ†ç±»æ¦‚ç‡åˆ†å¸ƒ: [ç›®æ ‡ç±»æ¦‚ç‡, éç›®æ ‡ç±»æ¦‚ç‡ä¹‹å’Œ]
    pred_student_tckd = torch.cat([
        pred_student.gather(1, target.unsqueeze(1)),
        (pred_student * other_mask).sum(dim=1, keepdim=True)
    ], dim=1)
    
    pred_teacher_tckd = torch.cat([
        pred_teacher.gather(1, target.unsqueeze(1)),
        (pred_teacher * other_mask).sum(dim=1, keepdim=True)
    ], dim=1)
    
    # è®¡ç®— TCKD çš„ KL æ•£åº¦
    log_pred_student_tckd = torch.log(pred_student_tckd + 1e-8)
    tckd_loss = F.kl_div(log_pred_student_tckd, pred_teacher_tckd, reduction='batchmean') * (temperature**2)
    
    # ----------- NCKD (Non-Target Class Knowledge Distillation) -----------
    # æ„é€ éç›®æ ‡ç±»çš„æ¦‚ç‡åˆ†å¸ƒ (æ’é™¤ç›®æ ‡ç±»åé‡æ–°å½’ä¸€åŒ–)
    # æŠ€å·§: å‡å»å¤§æ•°å±è”½ç›®æ ‡ç±»ï¼Œç„¶ååš Softmax
    pred_student_nckd = F.softmax(logits_student / temperature - 1000.0 * gt_mask, dim=1)
    pred_teacher_nckd = F.softmax(logits_teacher / temperature - 1000.0 * gt_mask, dim=1)
    
    # è®¡ç®— NCKD çš„ KL æ•£åº¦
    log_pred_student_nckd = torch.log(pred_student_nckd + 1e-8)
    nckd_loss = F.kl_div(log_pred_student_nckd, pred_teacher_nckd, reduction='batchmean') * (temperature**2)
    
    return alpha * tckd_loss + beta * nckd_loss

# ==================== æ•°æ®åŠ è½½ ====================
def get_data_loaders(config):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    transform = transforms.Compose([
        transforms.Resize((config.IMG_SIZE, config.IMG_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    train_dataset = datasets.ImageFolder(root=config.TRAIN_DIR, transform=transform)
    valid_dataset = datasets.ImageFolder(root=config.VALID_DIR, transform=transform)
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config.BATCH_SIZE, 
        shuffle=True, 
        num_workers=config.NUM_WORKERS, 
        pin_memory=config.PIN_MEMORY
    )
    
    valid_loader = DataLoader(
        valid_dataset, 
        batch_size=config.BATCH_SIZE, 
        shuffle=False, 
        num_workers=config.NUM_WORKERS, 
        pin_memory=config.PIN_MEMORY
    )
    
    return train_loader, valid_loader, train_dataset.classes

# ==================== è®­ç»ƒå•ä¸ªEpoch ====================
def train_one_epoch_distill(student, teacher, train_loader, optimizer, device, epoch, config):
    """è®­ç»ƒä¸€ä¸ªepoch (ä½¿ç”¨ DKD)"""
    student.train()
    teacher.eval() # æ•™å¸ˆæ¨¡å‹å§‹ç»ˆä¸ºè¯„ä¼°æ¨¡å¼
    
    running_loss = 0.0
    correct = 0
    total = 0
    
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{config.EPOCHS} [Train]", leave=False, ncols=100)
    
    for inputs, labels in pbar:
        inputs, labels = inputs.to(device), labels.to(device)
        
        optimizer.zero_grad()
        
        # æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­ (ä¸è®¡ç®—æ¢¯åº¦)
        with torch.no_grad():
            teacher_logits = teacher(inputs)
            
        # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
        student_logits = student(inputs)
        
        # 1. è®¡ç®—åŸºç¡€ CrossEntropy æŸå¤± (Hard Label)
        loss_ce = F.cross_entropy(student_logits, labels)
        
        # 2. è®¡ç®— DKD æŸå¤± (Soft Label)
        loss_dkd = dkd_loss(
            student_logits, 
            teacher_logits, 
            labels, 
            alpha=config.DKD_ALPHA, 
            beta=config.DKD_BETA, 
            temperature=config.TEMPERATURE
        )
        
        # æ€»æŸå¤±
        loss = loss_ce + loss_dkd
        
        loss.backward()
        optimizer.step()
        
        # ç»Ÿè®¡
        running_loss += loss.item() * inputs.size(0)
        _, preds = torch.max(student_logits, 1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)
        
        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100*correct/total:.2f}%'})
        
    epoch_loss = running_loss / total
    epoch_acc = 100 * correct / total
    
    return epoch_loss, epoch_acc

# ==================== éªŒè¯ ====================
def validate(model, valid_loader, device, epoch, config):
    """éªŒè¯æ¨¡å‹"""
    model.eval()
    
    running_loss = 0.0
    correct = 0
    total = 0
    
    all_preds = []
    all_labels = []
    
    pbar = tqdm(valid_loader, desc=f"Epoch {epoch}/{config.EPOCHS} [Valid]", leave=False, ncols=100)
    
    with torch.no_grad():
        for inputs, labels in pbar:
            inputs, labels = inputs.to(device), labels.to(device)
            
            logits = model(inputs)
            loss = F.cross_entropy(logits, labels)
            
            running_loss += loss.item() * inputs.size(0)
            _, preds = torch.max(logits, 1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            
            pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100*correct/total:.2f}%'})
            
    epoch_loss = running_loss / total
    epoch_acc = 100 * correct / total
    
    return epoch_loss, epoch_acc, all_preds, all_labels

# ==================== ä¸»è®­ç»ƒæµç¨‹ ====================
def train_distill(config):
    # åˆ›å»ºç›®å½•
    os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)
    os.makedirs(config.INDICATOR_DIR, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    train_loader, valid_loader, class_names = get_data_loaders(config)

    # åŠ è½½æ•™å¸ˆæ¨¡å‹
    print(f"\næ­£åœ¨åŠ è½½æ•™å¸ˆæ¨¡å‹: {config.TEACHER_CKPT}")
    teacher = LitenetResNet(num_classes=config.NUM_CLASSES, freeze_backbone=False).to(config.DEVICE)
    
    if os.path.exists(config.TEACHER_CKPT):
        teacher_ckpt = torch.load(config.TEACHER_CKPT, map_location=config.DEVICE)
        # å¤„ç†å¯èƒ½å­˜åœ¨çš„ 'model_state_dict' é”®
        if 'model_state_dict' in teacher_ckpt:
            teacher.load_state_dict(teacher_ckpt['model_state_dict'])
        else:
            teacher.load_state_dict(teacher_ckpt)
        print("âœ“ æ•™å¸ˆæ¨¡å‹åŠ è½½æˆåŠŸ")
    else:
        raise FileNotFoundError(f"æ•™å¸ˆæ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {config.TEACHER_CKPT}")
        
    teacher.eval()
    for p in teacher.parameters():
        p.requires_grad = False

    # åˆ›å»ºå­¦ç”Ÿæ¨¡å‹
    print("æ­£åœ¨åˆ›å»ºå­¦ç”Ÿæ¨¡å‹ (Litenet)...")
    student = Litenet(num_classes=config.NUM_CLASSES).to(config.DEVICE)
    
    # å®šä¹‰ä¼˜åŒ–å™¨
    optimizer = optim.Adam(student.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)
    
    # å®šä¹‰å­¦ä¹ ç‡è°ƒåº¦å™¨
    # scheduler = optim.lr_scheduler.CosineAnnealingLR(
    #     optimizer, T_max=config.EPOCHS, eta_min=config.MIN_LR
    # )

    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=10)

    
    # åˆå§‹åŒ–æ—©åœæœºåˆ¶
    early_stopping = EarlyStopping(patience=config.PATIENCE, verbose=True)

    # è®­ç»ƒå†å²è®°å½•
    train_losses = []
    train_accs = []
    val_losses = []
    val_accs = []
    lr_history = []
    
    best_acc = 0.0
    best_epoch = 0
    best_val_preds = []
    best_val_labels = []
    
    print("=" * 80)
    print("å¼€å§‹ DKD è’¸é¦è®­ç»ƒ...")
    print(f"æ•™å¸ˆ: ResNet18 | å­¦ç”Ÿ: Litenet")
    print(f"å‚æ•°: Temp={config.TEMPERATURE} | Alpha(TCKD)={config.DKD_ALPHA} | Beta(NCKD)={config.DKD_BETA}")
    print("=" * 80 + "\n")
    
    start_time = time.time()
    
    for epoch in range(1, config.EPOCHS + 1):
        epoch_start_time = time.time()
        
        # è®­ç»ƒ
        train_loss, train_acc = train_one_epoch_distill(
            student, teacher, train_loader, optimizer, config.DEVICE, epoch, config
        )
        
        # éªŒè¯
        val_loss, val_acc, val_preds, val_labels = validate(
            student, valid_loader, config.DEVICE, epoch, config
        )
        
        # è®°å½•å†å²
        train_losses.append(train_loss)
        train_accs.append(train_acc)
        val_losses.append(val_loss)
        val_accs.append(val_acc)
        lr_history.append(optimizer.param_groups[0]['lr'])
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step(val_acc) # ä¼ å…¥éªŒè¯å‡†ç¡®ç‡
        
        # è®¡ç®—è€—æ—¶
        epoch_time = time.time() - epoch_start_time
        
        # æ‰“å°ç»“æœ
        print(f"\nEpoch [{epoch}/{config.EPOCHS}] - ç”¨æ—¶: {epoch_time:.2f}s")
        print(f"è®­ç»ƒ - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%")
        print(f"éªŒè¯ - Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%")
        print(f"å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}")
        
        # æ—©åœæ£€æŸ¥
        early_stopping(val_acc)
        if early_stopping.early_stop:
            print("\n" + "=" * 30)
            print(f"æ—©åœè§¦å‘! éªŒè¯é›†å‡†ç¡®ç‡åœ¨ {config.PATIENCE} ä¸ªEpochå†…æœªæå‡")
            print("=" * 30 + "\n")
            break
        
        # æ¯5ä¸ªepochå®æ—¶æ›´æ–°å¯è§†åŒ–å›¾è¡¨
        if epoch % 5 == 0:
            print(f"\nğŸ“Š æ­£åœ¨æ›´æ–°å¯è§†åŒ–å›¾è¡¨ (Epoch {epoch})...")
            
            # æ›´æ–°è®­ç»ƒæ›²çº¿
            plot_training_curves(
                train_losses, train_accs, val_losses, val_accs, 
                save_path=config.INDICATOR_DIR
            )
            
            # æ›´æ–°æ··æ·†çŸ©é˜µ
            plot_confusion_matrix(
                val_labels, val_preds, class_names, 
                save_path=config.INDICATOR_DIR
            )
            
            # æ›´æ–°å„ç±»è¯†åˆ«å‡†ç¡®ç‡
            plot_per_class_accuracy(
                val_labels, val_preds, class_names,
                save_path=config.INDICATOR_DIR
            )
            print(f"âœ“ å¯è§†åŒ–å›¾è¡¨å·²æ›´æ–°\n")
        
        # ä¿å­˜æœ€ä¼˜æ¨¡å‹
        if val_acc > best_acc:
            best_acc = val_acc
            best_epoch = epoch
            best_val_preds = val_preds
            best_val_labels = val_labels
            
            torch.save({
                'epoch': epoch, 
                'model_state_dict': student.state_dict(), 
                'optimizer_state_dict': optimizer.state_dict(),
                'best_acc': best_acc,
                'class_names': class_names
            }, os.path.join(config.CHECKPOINT_DIR, 'best_model_distill.pth'))
            
            print(f"âœ“ æ–°çš„æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜! éªŒè¯å‡†ç¡®ç‡: {best_acc:.2f}%")
        
    
    # è®­ç»ƒå®Œæˆ
    total_time = time.time() - start_time
    print("\n" + "=" * 80)
    print("è’¸é¦è®­ç»ƒå®Œæˆï¼")
    print("=" * 80)
    print(f"æ€»ç”¨æ—¶: {total_time / 60:.2f} åˆ†é’Ÿ")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.2f}% (Epoch {best_epoch})")
    print("=" * 80 + "\n")
    
    # ç”Ÿæˆé…ç½®å­—å…¸ç”¨äºæ—¥å¿—
    config_dict = {
        'MODEL': "Litenet (DKD Distilled from ResNet18)",
        'TEACHER': "ResNet18",
        'TEMPERATURE': config.TEMPERATURE,
        'DKD_ALPHA': config.DKD_ALPHA,
        'DKD_BETA': config.DKD_BETA,
        'EPOCHS': config.EPOCHS,
        'BATCH_SIZE': config.BATCH_SIZE,
        'LEARNING_RATE': config.LEARNING_RATE,
        'WEIGHT_DECAY': config.WEIGHT_DECAY,
        'OPTIMIZER': 'Adam',
        'LR_SCHEDULER': config.LR_SCHEDULER,
        'IMG_SIZE': config.IMG_SIZE,
        'NUM_CLASSES': config.NUM_CLASSES,
        'DEVICE': str(config.DEVICE),
        'TOTAL_TRAINING_TIME': f'{total_time / 60:.2f} min',
        'TRAIN_SAMPLES': len(train_loader.dataset),
        'VALID_SAMPLES': len(valid_loader.dataset),
    }
    
    # ç”Ÿæˆæœ€ç»ˆçš„æ‰€æœ‰å¯è§†åŒ–ç»“æœ
    visualize_all(
        y_true=best_val_labels,
        y_pred=best_val_preds,
        class_names=class_names,
        train_losses=train_losses,
        train_accs=train_accs,
        val_losses=val_losses,
        val_accs=val_accs,
        config=config_dict,
        best_acc=best_acc,
        best_epoch=best_epoch,
        lr_history=lr_history,
        save_path=config.INDICATOR_DIR
    )

if __name__ == "__main__":
    config = DistillConfig()
    train_distill(config)